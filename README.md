![Analytics](images/data_analytics_image2.png)

## Catresa's Projects

### [Project 1: Natural Language Processing - E-cigarette Subreddits: Project Overview](https://github.com/cbarlow6/Natural_Language_Processing)
Tobacco companies employ social media to market their products to youth and young adults. Cessation and prevention campaigns require an understanding of how this population communicates about and uses these products. A key challenge for surveillance of the products and understanding their patterns of use is the diverse and nonstandard nomenclature for the devices (Alexander et al. 2016). These devices are referred to, by the companies themselves, and by consumers, as “e-cigarettes,” “e-cigs,” “cigalikes,” “e-hookahs,” “mods,” “vape pens,” “vapes,” and “tank systems.” 

We hope to gain insight into tobacco marketing and patterns of use by analyzing social media platforms. Our project focuses on smoking among youth and young adults, and we believe Reddit is a platform used by this group. We wish to learn how tobacco companies communicate with this population and how this population communicates among itself on the subject of e-cigarettes. 

![WordCloud](images/wordcloud5.png)

![WordCorrelation](images/wordcorrelation5.png)

### [Project 2: Data Wrangling - 20newgroups: Project Overview](https://github.com/cbarlow6/newsgroups)
 The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly 
 across 20 different newsgroups. http://qwone.com/~jason/20Newsgroups/

As a graduate research assistant, I used the 20news-bydate-test dataset to clean and transform text documents for a text mining project. [Developing insights from social media using semantic lexical chains to mine short text structures](https://www.sciencedirect.com/science/article/pii/S016792361930171X)
* Read all 7532 texts documents from 20 topic folders into one dataframe using R. 
* Remove new lines, tabs, & carriage returns 
* Transform to necessary format (topic, post_no, text) 
* Write dataframe to csv

![newsgroups_excel](images/newsgroup_excel_file.png)
